{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "nAY0TspmH_jF"
      },
      "id": "nAY0TspmH_jF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6373dd",
      "metadata": {
        "id": "cb6373dd"
      },
      "outputs": [],
      "source": [
        "#import default and other essential libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import joblib\n",
        "import pickle\n",
        "import csv\n",
        "import sys\n",
        "import random\n",
        "import seaborn as sns\n",
        "from functools import reduce\n",
        "import matplotlib.backends.backend_pdf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "#Packages to split data and other preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.base import BaseEstimator,TransformerMixin\n",
        "from sklearn.decomposition import PCA \n",
        "from boruta import BorutaPy\n",
        "from imblearn.pipeline import Pipeline as sample_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "#Import Classifiers\n",
        "from sklearn import svm\n",
        "import smote_variants as sv\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "## Package for calculating accuracy and analysis\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import LeaveOneOut \n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, roc_auc_score, cohen_kappa_score, f1_score, precision_score, recall_score, matthews_corrcoef \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from lime.lime_tabular import LimeTabularExplainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e38e010",
      "metadata": {
        "id": "2e38e010"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc0cf92",
      "metadata": {
        "id": "2bc0cf92"
      },
      "outputs": [],
      "source": [
        "#load preprocessed feature file here\n",
        "data = pd.read_csv(r'sign_electrophile_latest_preprocessed.csv') ### features in columns,molecules in rows\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63521a7",
      "metadata": {
        "id": "b63521a7"
      },
      "outputs": [],
      "source": [
        " # dropping ALL duplicate values, if any\n",
        "data.drop_duplicates(subset =\"smiles\", keep = 'first', inplace = True, ignore_index = True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e99dfa",
      "metadata": {
        "id": "d8e99dfa"
      },
      "outputs": [],
      "source": [
        "#Drop smiles column from the data\n",
        "data=data.drop('smiles', axis=1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# look at class imbalance\n",
        "data['status'].value_counts()"
      ],
      "metadata": {
        "id": "j8JTAQ6eRn3O"
      },
      "id": "j8JTAQ6eRn3O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "35594318",
      "metadata": {
        "id": "35594318"
      },
      "source": [
        "# Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb25556",
      "metadata": {
        "id": "2fb25556"
      },
      "outputs": [],
      "source": [
        "# Split Data\n",
        "X_train, X_test,y_train,y_test = train_test_split(data,data[\"status\"] ,test_size=0.25, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47bdeaa6",
      "metadata": {
        "id": "47bdeaa6"
      },
      "outputs": [],
      "source": [
        "train_df_new = X_train.drop('status', axis=1)\n",
        "valid_df_new = X_test.drop('status', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection (Boruta)"
      ],
      "metadata": {
        "id": "EU_t91VeXaMI"
      },
      "id": "EU_t91VeXaMI"
    },
    {
      "cell_type": "code",
      "source": [
        "def Boruta_Filteration(X_train,y_train,X_test,y_test):\n",
        "    #### making files for boruta\n",
        "    features = [f for f in X_train.columns if f not in ['status']]\n",
        "    X_train_boruta = X_train[features].values\n",
        "    Y_train_boruta = y_train.values.ravel()\n",
        "    X_test_boruta = X_test[features].values\n",
        "    Y_test_boruta = y_test.values.ravel()\n",
        "\n",
        "    print('Before filteration\\nTrain shape\\n',X_train_boruta.shape,'\\nTest shape\\n',X_test_boruta.shape)\n",
        "\n",
        "    ### implementing boruta\n",
        "    \n",
        "    # define random forest classifier, with utilising all cores and\n",
        "    # sampling in proportion to y labels\n",
        "    rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
        "\n",
        "    # define Boruta feature selection method\n",
        "    feat_selector = BorutaPy(rf, n_estimators=100, random_state=1)\n",
        "\n",
        "    # find all relevant features - 5 features should be selected\n",
        "    feat_selector.fit(X_train_boruta, Y_train_boruta)\n",
        "\n",
        "    # check selected features - first 5 features are selected\n",
        "    feat_selector.support_\n",
        "\n",
        "    # check ranking of features\n",
        "    feat_selector.ranking_\n",
        "\n",
        "    # call transform() on X to filter it down to selected features\n",
        "    X_train_filtered = feat_selector.transform(X_train_boruta)\n",
        "    X_test_filtered = feat_selector.transform(X_test_boruta)\n",
        "\n",
        "    ### name of the features selected####\n",
        "    final_features = list()\n",
        "    indexes = np.where(feat_selector.support_ == True)\n",
        "    for x in np.nditer(indexes):\n",
        "        final_features.append(features[x])\n",
        "    \n",
        "    print('# of Features selected:',len(final_features))\n",
        "\n",
        "    X_train_filtered=pd.DataFrame(X_train_filtered,columns=final_features)\n",
        "    X_test_filtered=pd.DataFrame(X_test_filtered,columns=final_features)\n",
        "\n",
        "    print('After filteration\\nTrain shape\\n',X_train_filtered.shape,'\\nTest shape\\n',X_test_filtered.shape)\n",
        "\n",
        "    return X_train_filtered,X_test_filtered,Y_train_boruta,Y_test_boruta,final_features"
      ],
      "metadata": {
        "id": "4uXyxldcXgFw"
      },
      "id": "4uXyxldcXgFw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6be52c6c",
      "metadata": {
        "id": "6be52c6c"
      },
      "source": [
        "# tsne Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7af0d91",
      "metadata": {
        "id": "b7af0d91"
      },
      "outputs": [],
      "source": [
        "def TSNE_plot(data,data_labels):\n",
        "        tsne = TSNE(n_components=2, random_state=50)\n",
        "        transformed_data = tsne.fit_transform(data)\n",
        "        k = np.array(transformed_data)\n",
        "        Group=[\"Class 0\",\"Class 1\"]\n",
        "        plt.scatter(k[:, 0],k[:, 1], c=data_labels)\n",
        "        #plt.legend(loc=\"lower right\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5158be55",
      "metadata": {
        "id": "5158be55"
      },
      "outputs": [],
      "source": [
        "TSNE_plot(train_df_new,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ed2f715",
      "metadata": {
        "id": "8ed2f715"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4e8490",
      "metadata": {
        "id": "5b4e8490"
      },
      "source": [
        "# Upsampling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6449e6db",
      "metadata": {
        "id": "6449e6db"
      },
      "outputs": [],
      "source": [
        "def Smote(traindata,trainlabel,prop):\n",
        "        oversampler= sv.MSMOTE(proportion=prop,random_state=50)\n",
        "        X_samp, y_samp= oversampler.sample(traindata.values,trainlabel.values)     \n",
        "        TSNE_plot(X_samp, y_samp)\n",
        "        X_samp= pd.DataFrame(X_samp)\n",
        "        y_samp=pd.DataFrame(y_samp)\n",
        "        X_samp.columns =list(traindata.columns.values)\n",
        "        return X_samp,y_samp\n",
        "def TSNE_plot(data,data_labels):\n",
        "        tsne = TSNE(n_components=2, random_state=50)\n",
        "        transformed_data = tsne.fit_transform(data)\n",
        "        k = np.array(transformed_data)\n",
        "        Group=[\"Class 0\",\"Class 1\"]\n",
        "        plt.scatter(k[:, 0],k[:, 1], c=data_labels)\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7249c1",
      "metadata": {
        "id": "7d7249c1"
      },
      "outputs": [],
      "source": [
        "X_train_filtered,Y_train_boruta=Smote(X_train_filtered,Y_train_boruta,0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71111371",
      "metadata": {
        "id": "71111371"
      },
      "outputs": [],
      "source": [
        "Y_train_boruta.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Down Sampling (For genomic instability)"
      ],
      "metadata": {
        "id": "t_tsSVtVSPvd"
      },
      "id": "t_tsSVtVSPvd"
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "def Smote(traindata,trainlabel):\n",
        "        # define undersample strategy\n",
        "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "        # fit and apply the transform\n",
        "        X_samp, y_samp = undersample.fit_resample(traindata.values,trainlabel.values)        \n",
        "        TSNE_plot(X_samp, y_samp)\n",
        "        X_samp= pd.DataFrame(X_samp)\n",
        "        y_samp=pd.DataFrame(y_samp)\n",
        "        X_samp.columns =list(traindata.columns.values)\n",
        "        return X_samp,y_samp\n",
        "def TSNE_plot(data,data_labels):\n",
        "        tsne = TSNE(n_components=2, random_state=50)\n",
        "        transformed_data = tsne.fit_transform(data)\n",
        "        k = np.array(transformed_data)\n",
        "        Group=[\"Class 0\",\"Class 1\"]\n",
        "        plt.scatter(k[:, 0],k[:, 1], c=data_labels)\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "OFiR9dOlSUhy"
      },
      "id": "OFiR9dOlSUhy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_filtered,Y_train_boruta=Smote(X_train_filtered,Y_train_boruta)"
      ],
      "metadata": {
        "id": "73JOX9I5Srd_"
      },
      "id": "73JOX9I5Srd_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_boruta.value_counts()"
      ],
      "metadata": {
        "id": "yYI02xzfSwch"
      },
      "id": "yYI02xzfSwch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Training and HyperParameter Tuning** \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Zn_arzibVfrg"
      },
      "id": "Zn_arzibVfrg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "KRGy060OYC5b"
      },
      "id": "KRGy060OYC5b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed to maintain the randomness of each hyperparameter tuning run\n",
        "def seed_all():\n",
        "    np.random.seed(123)\n",
        "    tf.random.set_seed(123)\n",
        "seed_all()"
      ],
      "metadata": {
        "id": "r0liHuRZYOBo"
      },
      "id": "r0liHuRZYOBo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new directory for HyperParameter Tuning\n",
        "os.mkdir('HPTuning')"
      ],
      "metadata": {
        "id": "YiGQDpIxYT6J"
      },
      "id": "YiGQDpIxYT6J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FROM = Path of directory from which to load the preprocessed signaturizer file\n",
        "FROM='/PreProcessed/'\n",
        "# TO = Path of the newly made HyperParameter Tuning directory\n",
        "TO='/HPTuning/'\n",
        "\n",
        "# Set the HPTuning directory as the current working directory\n",
        "os.chdir(TO)"
      ],
      "metadata": {
        "id": "Yd0YqLgwYgUL"
      },
      "id": "Yd0YqLgwYgUL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the preprocessed signaturizer file\n",
        "Data=pd.read_csv(FROM+'sign_proliferative_anti_preprocessed.csv')\n",
        "Data"
      ],
      "metadata": {
        "id": "ekibh2spYnLL"
      },
      "id": "ekibh2spYnLL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use 90% of the Data as Training Data for further hyperparameter tuning\n",
        "Train=Data.sample(n=int(len(Data)*0.9), random_state=1)\n",
        "Train"
      ],
      "metadata": {
        "id": "LD9dkwmAZavN"
      },
      "id": "LD9dkwmAZavN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Grid"
      ],
      "metadata": {
        "id": "2qaMOxhtZz86"
      },
      "id": "2qaMOxhtZz86"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Grid (Proliferation)"
      ],
      "metadata": {
        "id": "BAxEJz9sZ5NJ"
      },
      "id": "BAxEJz9sZ5NJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def HPTing_Model(Train_x, Train_y):\n",
        "    rf = RandomForestClassifier()\n",
        "    parameters = {\n",
        "        'max_features': ['auto', 'sqrt'],\n",
        "        'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'bootstrap':[True, False],\n",
        "        'n_estimators':[int(x) for x in np.linspace(start = 2, stop = 100, num = 10)]\n",
        "    }\n",
        "    grid = RandomizedSearchCV(rf, parameters, scoring='accuracy', return_train_score=False, cv =5)\n",
        "    grid_search=grid.fit(Train_x, Train_y)\n",
        "    return grid_search"
      ],
      "metadata": {
        "id": "_Qkd8K25aAEj"
      },
      "id": "_Qkd8K25aAEj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Grid (Electrophile)"
      ],
      "metadata": {
        "id": "ynfEpHyCZ9kp"
      },
      "id": "ynfEpHyCZ9kp"
    },
    {
      "cell_type": "code",
      "source": [
        "def HPTing_Model(Train_x, Train_y):\n",
        "    mlp = MLPClassifier()\n",
        "    parameter_space = { 'hidden_layer_sizes':[(5,5,5),(20,30,50),(50,50,50), (50,100,50), (100,),(100,100,100),(5,2)],\n",
        "                       'activation': ['tanh', 'relu'],\n",
        "                       'solver': ['sgd', 'adam'],\n",
        "                       'alpha': [0.001, 0.01, 0.02, 0.04, 0.05],\n",
        "                       'learning_rate': ['constant','adaptive','invscaling']\n",
        "}\n",
        "    grid = RandomizedSearchCV(mlp, parameter_space, scoring='accuracy', return_train_score=False, cv =5)\n",
        "    grid_search=grid.fit(Train_x, Train_y)\n",
        "    return grid_search"
      ],
      "metadata": {
        "id": "dOh11DJCcBmB"
      },
      "id": "dOh11DJCcBmB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Grid (Oxidative)"
      ],
      "metadata": {
        "id": "Ue3oAch1cOlx"
      },
      "id": "Ue3oAch1cOlx"
    },
    {
      "cell_type": "code",
      "source": [
        "def HPTing_Model(Train_x, Train_y):\n",
        "    mlp = MLPClassifier( max_iter= 1000 , random_state=50)\n",
        "    parameter_space = { 'hidden_layer_sizes':[(5,5,5),(20,30,50),(50,50,50), (50,100,50), (100,),(100,100,100),(5,2)],'activation': ['tanh', 'relu'],\n",
        "        'solver': ['sgd', 'adam'],\n",
        "        'alpha': [0.0001, 0.05,0.001,0.01],\n",
        "        'learning_rate': ['constant','adaptive']}\n",
        "    grid = RandomizedSearchCV(mlp, parameter_space, scoring='accuracy',cv=5 ,return_train_score=False)\n",
        "    grid_search=grid.fit(Train_x, Train_y)\n",
        "    return grid_search"
      ],
      "metadata": {
        "id": "G_S7JNtqcSQ6"
      },
      "id": "G_S7JNtqcSQ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM Grid (Epigenetics)"
      ],
      "metadata": {
        "id": "J026ZL7Bcz7x"
      },
      "id": "J026ZL7Bcz7x"
    },
    {
      "cell_type": "code",
      "source": [
        "def HPTing_Model(Train_x, Train_y):\n",
        "    svc_rand = SVC(probability=True)\n",
        "    parameters = {\n",
        "        'kernel':('linear', 'rbf'),\n",
        "        'C': [0.5, 0.6, 0.8, 1.0, 1.2, 1.5], \n",
        "        'gamma': [0.05, 0.1, 1.0, 1.2, 1.5, 2],\n",
        "    }\n",
        "    grid = RandomizedSearchCV(svc_rand, parameters, cv = 5)\n",
        "    grid_search=grid.fit(Train_x, Train_y)\n",
        "    return grid_search"
      ],
      "metadata": {
        "id": "z9sw3UGlc3xL"
      },
      "id": "z9sw3UGlc3xL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Grid (Genomic Instability)"
      ],
      "metadata": {
        "id": "VzFDOLM-c6VT"
      },
      "id": "VzFDOLM-c6VT"
    },
    {
      "cell_type": "code",
      "source": [
        "def HPTing_Model(Train_x, Train_y):\n",
        "    n_estimators = [int(x) for x in np.linspace(start = 2, stop = 100, num = 10)]\n",
        "    max_features = ['auto', 'sqrt']\n",
        "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "    max_depth.append(None)\n",
        "    min_samples_split = list(range(1,30))\n",
        "    min_samples_leaf = list(range(1,20))\n",
        "    bootstrap = [True, False]\n",
        "    random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "    rf = RandomForestClassifier()\n",
        "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=2, n_jobs = -1)\n",
        "    rf_random.fit(Train_x, Train_y)\n",
        "    return rf_random"
      ],
      "metadata": {
        "id": "c6q8TpU-dAAC"
      },
      "id": "c6q8TpU-dAAC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN Grid (Apoptosis)"
      ],
      "metadata": {
        "id": "lGSVfSxJdCnp"
      },
      "id": "lGSVfSxJdCnp"
    },
    {
      "cell_type": "code",
      "source": [
        "def HPTing_Model(Train_x, Train_y):\n",
        "    leaf_size = list(range(1,50))\n",
        "    n_neighbors = list(range(1,40))\n",
        "    p=list(range(1,20))\n",
        "    #Convert to dictionary\n",
        "    hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p, weights=['uniform','distance'], metric= ['minkowski','euclidean','manhattan'])\n",
        "    #Create new KNN object\n",
        "    knn = KNeighborsClassifier()\n",
        "    #Use GridSearch\n",
        "    knn_Grid = GridSearchCV(knn, hyperparameters, cv=3, verbose=2, n_jobs = -1)\n",
        "    best_model = knn_Grid.fit(Train_x, Train_y)\n",
        "    return best_model\n"
      ],
      "metadata": {
        "id": "YrqNmFQmdFcB"
      },
      "id": "YrqNmFQmdFcB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Labels Function"
      ],
      "metadata": {
        "id": "mai1HWTZdGMZ"
      },
      "id": "mai1HWTZdGMZ"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(pred_test,thsd): #Getting discrete labels from probability values    \n",
        "    test_label = [] \n",
        "    for i in range(len(pred_test)):\n",
        "        if pred_test[i]>thsd:\n",
        "            test_label.append(1)\n",
        "        else:\n",
        "            test_label.append(0)\n",
        "    return test_label"
      ],
      "metadata": {
        "id": "0D_Zcu65dWV4"
      },
      "id": "0D_Zcu65dWV4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scoring Metrics"
      ],
      "metadata": {
        "id": "-dudwC7DdJLy"
      },
      "id": "-dudwC7DdJLy"
    },
    {
      "cell_type": "code",
      "source": [
        "def Scoring_metrices(label, pred, truth, D):\n",
        "    score={}\n",
        "    \n",
        "    accuracy = metrics.accuracy_score(truth, label)\n",
        "    score[D+\" Accuracy:\"] = accuracy\n",
        "    print(D+\" Accuracy:\", accuracy)\n",
        "    \n",
        "    mcc_score = matthews_corrcoef(truth, label)\n",
        "    score[D+\" MCC Score:\"] = mcc_score\n",
        "    print(D+\" MCC Score:\",mcc_score)\n",
        "    \n",
        "    F1_score = f1_score(truth, label, average='macro')\n",
        "    score[D+\" F1 Score:\"] = F1_score\n",
        "    print(D+\" F1 Score:\", F1_score)\n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(truth, pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    score[D+\" AUC VALUE:\"] = roc_auc\n",
        "    print(D+\" AUC VALUE:\",roc_auc)\n",
        "    \n",
        "    kappa_rf=sklearn.metrics.cohen_kappa_score(truth, label)\n",
        "    score[D+\" kappa Score:\"] = kappa_rf\n",
        "    print(D+\" kappa Score:\",kappa_rf)\n",
        "    \n",
        "    Precision_score = metrics.precision_score(truth, label)\n",
        "    score[D+\" Precision:\"] = Precision_score\n",
        "    print(D+\" Precision:\", Precision_score)\n",
        "    \n",
        "    Recall_score = metrics.recall_score(truth, label)\n",
        "    score[D+\" Recall:\"] = Recall_score\n",
        "    print(D+\" Recall:\", Recall_score)\n",
        "    \n",
        "    \n",
        "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=D)\n",
        "    display.plot()\n",
        "    #plt.savefig('AUC_ROC.pdf')\n",
        "    plt.show()\n",
        "    return score"
      ],
      "metadata": {
        "id": "VfiVsmV0dkJJ"
      },
      "id": "VfiVsmV0dkJJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Hyperparameter Tuning of the Model"
      ],
      "metadata": {
        "id": "8jJh7r2Kdm9P"
      },
      "id": "8jJh7r2Kdm9P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following chunks according to the selected model for a property"
      ],
      "metadata": {
        "id": "wXghGTH0Riei"
      },
      "id": "wXghGTH0Riei"
    },
    {
      "cell_type": "code",
      "source": [
        "Train_Fold_outs=[]\n",
        "Test_Fold_outs=[]\n",
        "Best_params=[]\n",
        "features = []\n",
        "models=[]\n",
        "\n",
        "#this chunk runs 20 iterations of the 5-Cross Validation of the pre-defined grid for the model\n",
        "for i in range(20):\n",
        "    print('Fold #',i)    \n",
        "    \n",
        "    # Split Data\n",
        "    X_train, X_test,y_train,y_test = train_test_split(Train,Train[\"status\"] ,test_size=0.25, shuffle = True, random_state=i)\n",
        "    \n",
        "    #Drop smiles,status from training,testing data\n",
        "    x_train = X_train.drop(['status','smiles'], axis=1)\n",
        "    x_test = X_test.drop(['status','smiles'], axis=1)\n",
        "\n",
        "    #Feature selection\n",
        "    x_train_filtered,x_test_filtered,y_train_filtered,y_test_filtered,selected_features = Boruta_Filteration(x_train,y_train,x_test,y_test)\n",
        "    features.append(selected_features)\n",
        "    \n",
        "    y_train_filtered = pd.Series(y_train_filtered)\n",
        "    \n",
        "    #Oversampling\n",
        "    Final_Xtrain,Final_Ytrain = Smote(x_train_filtered,y_train_filtered,0.5,'Upsampled')\n",
        "    \n",
        "    Final_Ytrain=Final_Ytrain.values.ravel()\n",
        "    Final_Xtrain = pd.DataFrame(Final_Xtrain, dtype = np.float64)\n",
        "    x_test_filtered = pd.DataFrame(x_test_filtered, dtype = np.float64)\n",
        "    \n",
        "    #Hyperparameter Tuning\n",
        "    Parameters = HPTing_Model(Final_Xtrain,Final_Ytrain)\n",
        "    \n",
        "    #save best parameters\n",
        "    Best_params.append(Parameters.best_estimator_.get_params())\n",
        "    \n",
        "    #build the tuned model\n",
        "    #edit the parameters here according to your defined parameter space and model's grid\n",
        "    rf = RandomForestClassifier(max_features=Parameters.best_estimator_.get_params()['max_features'],\n",
        "                        max_depth=Parameters.best_estimator_.get_params()['max_depth'],\n",
        "                        min_samples_split=Parameters.best_estimator_.get_params()['min_samples_split'],\n",
        "                        min_samples_leaf=Parameters.best_estimator_.get_params()['min_samples_leaf'],\n",
        "                        bootstrap=Parameters.best_estimator_.get_params()['bootstrap'],\n",
        "                        n_estimators=Parameters.best_estimator_.get_params()['n_estimators'])\n",
        "    \n",
        "    #fit the built model\n",
        "    rf.fit(Final_Xtrain,Final_Ytrain)\n",
        "    models.append(rf)\n",
        "\n",
        "    #Training Predictions for the model\n",
        "    y_train_pred=rf.predict(Final_Xtrain)\n",
        "    y_train_prob=rf.predict_proba(Final_Xtrain)\n",
        "\n",
        "    #Save training metrics\n",
        "    Train_Fold_outs.append(Scoring_metrices(y_train_pred,y_train_prob[:,1],Final_Ytrain,'Training'))\n",
        "\n",
        "    #Testing Predictions for the model\n",
        "    y_test_pred=rf.predict(x_test_filtered) \n",
        "    y_test_prob=rf.predict_proba(x_test_filtered)\n",
        "\n",
        "    #Save testing metrics\n",
        "    Test_Fold_outs.append(Scoring_metrices(y_test_pred,y_test_prob[:,1],y_test_filtered,'Testing'))"
      ],
      "metadata": {
        "id": "GgtTiXQYdtj5"
      },
      "id": "GgtTiXQYdtj5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse the training metrics sorted by descending Training Accuracy\n",
        "pd.DataFrame.from_dict(Train_Fold_outs).sort_values(by=['Training Accuracy:'],ascending = False)"
      ],
      "metadata": {
        "id": "eaYoKtjjfO4q"
      },
      "id": "eaYoKtjjfO4q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse the testing metrics sorted by descending Testing Accuracy\n",
        "pd.DataFrame.from_dict(Test_Fold_outs).sort_values(by=['Testing Accuracy:'],ascending = False)"
      ],
      "metadata": {
        "id": "q-N-_YJffRed"
      },
      "id": "q-N-_YJffRed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#View the best parameters\n",
        "pd.DataFrame.from_dict(Best_params)"
      ],
      "metadata": {
        "id": "MfyssdIifkhh"
      },
      "id": "MfyssdIifkhh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#View length of the features selected for the top performing/most stable (selected) model (1st here)\n",
        "len(features[1])"
      ],
      "metadata": {
        "id": "mcVISXs8fpyi"
      },
      "id": "mcVISXs8fpyi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the feature names\n",
        "pd.DataFrame(features[1]).to_csv('/HPTuning/anti_prol_features_rf.csv',index=False)"
      ],
      "metadata": {
        "id": "vZg6bqzaf5x0"
      },
      "id": "vZg6bqzaf5x0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the best parameters of the chosen model\n",
        "with open('/HPTuning/anti_prol_best_params_RF.csv', 'w') as f:  # You will need 'wb' mode in Python 2.x\n",
        "    w = csv.DictWriter(f, Best_params[1].keys())\n",
        "    w.writeheader()\n",
        "    w.writerow(Best_params[1])"
      ],
      "metadata": {
        "id": "Hcg_jl3xf8NC"
      },
      "id": "Hcg_jl3xf8NC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20 Fold Boosting\n"
      ],
      "metadata": {
        "id": "APvpfxQ1gPpp"
      },
      "id": "APvpfxQ1gPpp"
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly split the data into testing and validation for each fold\n",
        "def Test_valid_split(Set3,frac,seed):\n",
        "    Fraction=frac\n",
        "    Test=Set3[Set3['status']==1].sample(frac = Fraction,random_state=1).append(Set3[Set3['status']==0].sample(frac = Fraction,random_state=seed))\n",
        "    Valid_index=[item for item in list(Set3.index) if item not in list(Test.index)]\n",
        "    Valid=Set3.T[Valid_index].T\n",
        "    print('Test set size:',len(Test),'\\nValid set size:',len(Valid))\n",
        "    return Test,Valid"
      ],
      "metadata": {
        "id": "ofzgTQgNgTeK"
      },
      "id": "ofzgTQgNgTeK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_list=features[1] #feature list of the selected (hyperparameter tuned) model\n",
        "Train_Fold_outs_1=[]\n",
        "Test_Fold_outs_1=[]\n",
        "models_1=[]\n",
        "\n",
        "for i in range(20):\n",
        "    print('Fold #',i)\n",
        "    \n",
        "    #Train-test split randomly\n",
        "    Trn,Tst = Test_valid_split(Data,0.90,i)\n",
        "    Train_y, Test_y = Trn['status'],Tst['status']\n",
        "\n",
        "    #Use selected feature list\n",
        "    Train_x = Trn[f_list]\n",
        "    Test_x = Tst[f_list]\n",
        "    \n",
        "    x_train_filtered = Train_x.values\n",
        "    x_test_filtered = Test_x.values\n",
        "    y_train_filtered = Train_y.values.ravel()\n",
        "    y_test_filtered = Test_y.values.ravel()\n",
        "    \n",
        "    #Upsampling\n",
        "    Final_Xtrain,Final_Ytrain = Smote(Train_x,Train_y,0.5,'Upsamlped')\n",
        "    \n",
        "    Final_Ytrain=Final_Ytrain.values.ravel()\n",
        "    Final_Xtrain = pd.DataFrame(Final_Xtrain, dtype = np.float64)\n",
        "    x_test_filtered = pd.DataFrame(x_test_filtered, dtype = np.float64)\n",
        "\n",
        "    #Use the best parameters from the chosen model here\n",
        "    rf = RandomForestClassifier(bootstrap= True,\n",
        "                                ccp_alpha= 0.0,\n",
        "                                class_weight= None,\n",
        "                                criterion= 'gini',\n",
        "                                max_depth= 80,\n",
        "                                max_features= 'auto',\n",
        "                                max_leaf_nodes= None,\n",
        "                                max_samples= None,\n",
        "                                min_impurity_decrease= 0.0,\n",
        "                                min_samples_leaf= 1,\n",
        "                                min_samples_split= 10,\n",
        "                                min_weight_fraction_leaf= 0.0,\n",
        "                                n_estimators= 45,\n",
        "                                n_jobs= None,\n",
        "                                oob_score= False,\n",
        "                                random_state= None,\n",
        "                                verbose= 0,\n",
        "                                warm_start= False)\n",
        "    \n",
        "    #Fit the model\n",
        "    rf.fit(Final_Xtrain,Final_Ytrain)\n",
        "    models_1.append(rf)\n",
        "    \n",
        "    #Training prediction and saving the metrics\n",
        "    y_train_pred=rf.predict(Final_Xtrain)\n",
        "    y_train_prob=rf.predict_proba(Final_Xtrain)\n",
        "    Train_Fold_outs_1.append(Scoring_metrices(y_train_pred,y_train_prob[:,1],Final_Ytrain,'Training'))\n",
        "\n",
        "    #Testing prediction and saving the metrics\n",
        "    y_test_pred=rf.predict(x_test_filtered) \n",
        "    y_test_prob=rf.predict_proba(x_test_filtered)\n",
        "    Test_Fold_outs_1.append(Scoring_metrices(y_test_pred,y_test_prob[:,1],y_test_filtered.astype('int'),'Testing'))"
      ],
      "metadata": {
        "id": "9kCFAxwDglOj"
      },
      "id": "9kCFAxwDglOj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To visualize the stability of the tuned model with each fold\n",
        "(pd.DataFrame(Test_Fold_outs_1)).boxplot(grid=False,rot=45)"
      ],
      "metadata": {
        "id": "WpdmwcpohQDU"
      },
      "id": "WpdmwcpohQDU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To visualize the stability of the tuned model with each fold\n",
        "(pd.DataFrame(Train_Fold_outs_1)).boxplot(grid=False,rot=45)"
      ],
      "metadata": {
        "id": "Ka8pTMZ7hSmz"
      },
      "id": "Ka8pTMZ7hSmz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(Test_Fold_outs_1)"
      ],
      "metadata": {
        "id": "MXsGWZKKhgZh"
      },
      "id": "MXsGWZKKhgZh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(Train_Fold_outs_1)"
      ],
      "metadata": {
        "id": "J953l0lwhiEx"
      },
      "id": "J953l0lwhiEx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training on Whole Data "
      ],
      "metadata": {
        "id": "uq8EnV6XhpB5"
      },
      "id": "uq8EnV6XhpB5"
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN = Data.drop(['smiles','status'],axis=1)\n",
        "TRAIN"
      ],
      "metadata": {
        "id": "xEgGRKAmhukU"
      },
      "id": "xEgGRKAmhukU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN = TRAIN[features[1]] #Use features of the selected (hyperparameter tuned) model here\n",
        "TRAIN"
      ],
      "metadata": {
        "id": "EeYSukBVhwaT"
      },
      "id": "EeYSukBVhwaT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = Data['status']"
      ],
      "metadata": {
        "id": "1hoSYdQ9h7Dj"
      },
      "id": "1hoSYdQ9h7Dj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model on whole data\n",
        "fitted = models[1].fit(TRAIN,Y)\n",
        "fitted"
      ],
      "metadata": {
        "id": "2m14LAf8h_o9"
      },
      "id": "2m14LAf8h_o9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the final model\n",
        "joblib.dump(fitted, '/HPTuning/anti_prol_model_rf.pkl')"
      ],
      "metadata": {
        "id": "Uhi5NX3RjgKc"
      },
      "id": "Uhi5NX3RjgKc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MK Ensemble (Gradient Boosting Classifier)"
      ],
      "metadata": {
        "id": "0cO43sFWaHEj"
      },
      "id": "0cO43sFWaHEj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper parameter tunning"
      ],
      "metadata": {
        "id": "q4DqNudsh-DG"
      },
      "id": "q4DqNudsh-DG"
    },
    {
      "cell_type": "code",
      "source": [
        "def HPTing_Randomsearch(Train_x, Train_y):\n",
        "    n_estimators = [int(x) for x in np.linspace(start = 2, stop = 200, num = 20)]\n",
        "    learning_rate = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
        "    min_samples_split = list(range(2,40))\n",
        "    min_samples_leaf = list(range(1,30))\n",
        "    max_depth = list(range(1,15))\n",
        "    max_features = ['auto', 'sqrt']\n",
        "    random_grid = {'n_estimators': n_estimators,\n",
        "                   'learning_rate': learning_rate,\n",
        "                   'max_features': max_features,\n",
        "                   'max_depth': max_depth,\n",
        "                   'min_samples_split': min_samples_split,\n",
        "                   'min_samples_leaf': min_samples_leaf}\n",
        "    gbc=GradientBoostingClassifier()\n",
        "    gsearch1 = RandomizedSearchCV(estimator = gbc, param_distributions = random_grid, n_iter = 100, verbose=2, scoring='roc_auc', n_jobs=-1, cv=3) \n",
        "    gsearch1.fit(x_Train, y_Train)\n",
        "    return gsearch1\n"
      ],
      "metadata": {
        "id": "mVG64PFnaUYt"
      },
      "id": "mVG64PFnaUYt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=0\n",
        "gbc_Train_Fold_outs=[]\n",
        "gbc_Test_Fold_outs=[]\n",
        "gbc_DP4NC_Fold_outs=[]\n",
        "gbc_Best_params=[]\n",
        "gbc_models=[]\n",
        "\n",
        "for i in range(20):\n",
        "    f+=1\n",
        "    print('Fold #',f)\n",
        "    Train_x, Test_x, y_Train, y_Test = train_test_split(Data,Data[\"status\"] ,test_size=0.15, random_state=f)\n",
        "    Test_x = Test_x.filter(regex='_1$').rename(columns={'GInstability_1':'GI','Apoptosis_1':'Apo','Electrophile_1':'Elec','Proliferation_1':'Prf','Oxidative_1':'Oxd','Epigenetics_1':'Epig'})\n",
        "    Train_x = Train_x.filter(regex='_1$').rename(columns={'GInstability_1':'GI','Apoptosis_1':'Apo','Electrophile_1':'Elec','Proliferation_1':'Prf','Oxidative_1':'Oxd','Epigenetics_1':'Epig'})\n",
        "    x_Train=Fetaure_combos(Train_x.copy(),1).apply(pd.to_numeric)\n",
        "    x_Test=Fetaure_combos(Test_x.copy(),1).apply(pd.to_numeric)\n",
        "    y_Train=y_Train.apply(int)\n",
        "    y_Test=y_Test.apply(int)\n",
        "#     Final_Xtrain,Final_Ytrain = Smote(x_Train,y_Train,0.5,'Upsamlped')\n",
        "    Final_Xtrain=x_Train\n",
        "    Final_Ytrain=y_Train.values.ravel()\n",
        "    Parametrs = HPTing_Randomsearch(Final_Xtrain.apply(pd.to_numeric),Final_Ytrain)\n",
        "    gbc_Best_params.append(Parametrs.best_params_)\n",
        "    model_gbc = GradientBoostingClassifier(learning_rate= Parametrs.best_params_['learning_rate'],\n",
        "                                      max_depth= Parametrs.best_params_['max_depth'],\n",
        "                                      max_features= Parametrs.best_params_['max_features'],\n",
        "                                      min_samples_leaf= Parametrs.best_params_['min_samples_leaf'],\n",
        "                                      min_samples_split= Parametrs.best_params_['min_samples_split'],\n",
        "                                      n_estimators =Parametrs.best_params_['n_estimators'])\n",
        "    model_gbc.fit(Final_Xtrain.apply(pd.to_numeric),Final_Ytrain)\n",
        "    gbc_models.append(model_gbc)\n",
        "    y_train_pred=model_gbc.predict(Final_Xtrain.apply(pd.to_numeric))\n",
        "    y_train_prob=model_gbc.predict_proba(Final_Xtrain.apply(pd.to_numeric))\n",
        "    gbc_Train_Fold_outs.append(Scoring_metrices(y_train_pred,y_train_prob[:,1],Final_Ytrain,'Training'))\n",
        "    y_test_pred=model_gbc.predict(x_Test.apply(pd.to_numeric)) \n",
        "    y_test_prob=model_gbc.predict_proba(x_Test.apply(pd.to_numeric))\n",
        "    gbc_Test_Fold_outs.append(Scoring_metrices(y_test_pred,y_test_prob[:,1],y_Test,'Testing'))\n",
        "    gbc_DP4NC_Fold_outs.append(DP4NC(model_gbc,Test,Tst_x,'Fold'+str(f)))\n"
      ],
      "metadata": {
        "id": "G-El26A8h0Z7"
      },
      "id": "G-El26A8h0Z7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble models (20)"
      ],
      "metadata": {
        "id": "A2TGbVFeiEvb"
      },
      "id": "A2TGbVFeiEvb"
    },
    {
      "cell_type": "code",
      "source": [
        "f=0\n",
        "Cut5_Train_Fold_outs=[]\n",
        "Cut5_Test_Fold_outs=[]\n",
        "Cut5_DP4NC_Fold_outs=[]\n",
        "Cut5_models=[]\n",
        "\n",
        "for i in range(f,20):\n",
        "    f+=1\n",
        "    print('Fold #',f)\n",
        "    Train_x, Test_x, y_Train, y_Test = train_test_split(Data,Data[\"status\"] ,test_size=0.10, random_state=f)\n",
        "    Test_x = Test_x.filter(regex='_1$').rename(columns={'GInstability_1':'GI','Apoptosis_1':'Apo','Electrophile_1':'Elec','Proliferation_1':'Prf','Oxidative_1':'Oxd','Epigenetics_1':'Epig'})\n",
        "    Train_x = Train_x.filter(regex='_1$').rename(columns={'GInstability_1':'GI','Apoptosis_1':'Apo','Electrophile_1':'Elec','Proliferation_1':'Prf','Oxidative_1':'Oxd','Epigenetics_1':'Epig'})\n",
        "    x_Train=Fetaure_combos(Train_x.copy(),1).apply(pd.to_numeric)\n",
        "    x_Test=Fetaure_combos(Test_x.copy(),1).apply(pd.to_numeric)\n",
        "    y_Train=y_Train.apply(int)\n",
        "    y_Test=y_Test.apply(int)\n",
        "#     Final_Xtrain,Final_Ytrain = Smote(x_Train,y_Train,0.5,'Upsamlped')\n",
        "    Final_Xtrain=x_Train\n",
        "    Final_Ytrain=y_Train.values.ravel()\n",
        "    model_gbc = GradientBoostingClassifier(n_estimators = 22,\n",
        "                                       min_samples_split = 11,\n",
        "                                       min_samples_leaf = 6,\n",
        "                                       max_features = 'sqrt',\n",
        "                                       max_depth = 5,\n",
        "                                       learning_rate = 0.1)\n",
        "    model_gbc.fit(Final_Xtrain.apply(pd.to_numeric),Final_Ytrain)\n",
        "    Cut5_models.append(model_gbc)\n",
        "    y_train_pred=model_gbc.predict(Final_Xtrain.apply(pd.to_numeric))\n",
        "    y_train_prob=model_gbc.predict_proba(Final_Xtrain.apply(pd.to_numeric))\n",
        "    Cut5_Train_Fold_outs.append(Scoring_metrices(y_train_pred,y_train_prob[:,1],Final_Ytrain,'Training'))\n",
        "    y_test_pred=model_gbc.predict(x_Test.apply(pd.to_numeric)) \n",
        "    y_test_prob=model_gbc.predict_proba(x_Test.apply(pd.to_numeric))\n",
        "    Cut5_Test_Fold_outs.append(Scoring_metrices(y_test_pred,y_test_prob[:,1],y_Test,'Testing'))\n",
        "    Cut5_DP4NC_Fold_outs.append(DP4NC(model_gbc,Test,Tst_x,'Fold'+str(f)))"
      ],
      "metadata": {
        "id": "qiag9nabhRUq"
      },
      "id": "qiag9nabhRUq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "metabokiller",
      "language": "python",
      "name": "metabokiller"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Holy_Grail_ Final_ML_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nAY0TspmH_jF",
        "2e38e010",
        "35594318",
        "6be52c6c",
        "c813a6e8",
        "5b4e8490",
        "t_tsSVtVSPvd",
        "Zn_arzibVfrg",
        "EU_t91VeXaMI",
        "KRGy060OYC5b",
        "2qaMOxhtZz86",
        "BAxEJz9sZ5NJ",
        "ynfEpHyCZ9kp",
        "Ue3oAch1cOlx",
        "J026ZL7Bcz7x",
        "VzFDOLM-c6VT",
        "lGSVfSxJdCnp",
        "mai1HWTZdGMZ",
        "-dudwC7DdJLy",
        "8jJh7r2Kdm9P",
        "APvpfxQ1gPpp",
        "uq8EnV6XhpB5"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}